# train.py
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torcheval.metrics import MulticlassAccuracy

import argparse
import os
from data_preparation import create_few_shot_cifar10, build_cifar10_with_sd
from augmentation import create_augmented_dataset
from models_custom import get_resnet18_cifar10



def _make_loader(dataset, batch_size, train, num_workers=2):
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=train,
        num_workers=num_workers,
        pin_memory=True
    )


@torch.no_grad()
def _evaluate(model, loader, device):
    model.eval()
    metric = MulticlassAccuracy(num_classes=10).to(device)
    loss_sum, n = 0.0, 0
    for x, y in loader:
        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
        logits = model(x)
        loss = F.cross_entropy(logits, y)
        loss_sum += loss.item() * x.size(0)
        n += x.size(0)
        metric.update(logits, y)
    return loss_sum / n, metric.compute().item()


def train_classifier(model, train_dataset, test_dataset,
                     epochs=100, batch_size=128, lr=0.05,
                     device='cuda', save_path='./models/best.pth'):
    device = device if torch.cuda.is_available() else 'cpu'
    model = model.to(device)

    train_loader = _make_loader(train_dataset, batch_size, train=True)
    test_loader  = _make_loader(test_dataset,  batch_size, train=False)

    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9,
                                weight_decay=5e-4, nesterov=True)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))

    best_acc = 0.0
    history = {'train_acc': [], 'test_acc': [], 'train_loss': [], 'test_loss': [], 'best_acc': 0.0}

    for ep in range(1, epochs+1):
        model.train()
        metric = MulticlassAccuracy(num_classes=10).to(device)
        loss_sum, n = 0.0, 0

        for x, y in train_loader:
            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)
            optimizer.zero_grad(set_to_none=True)
            with torch.autocast(device_type=device, dtype=torch.float16, enabled=(device=='cuda')):
                logits = model(x)
                loss = F.cross_entropy(logits, y)
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            loss_sum += loss.item() * x.size(0)
            n += x.size(0)
            metric.update(logits, y)

        tr_loss = loss_sum / n
        tr_acc = metric.compute().item()
        te_loss, te_acc = _evaluate(model, test_loader, device)
        scheduler.step()

        if te_acc > best_acc:
            best_acc = te_acc
            torch.save(model.state_dict(), save_path)

        history['train_loss'].append(tr_loss)
        history['test_loss'].append(te_loss)
        history['train_acc'].append(tr_acc)
        history['test_acc'].append(te_acc)
        history['best_acc'] = best_acc

        print(f"[{ep:03d}] train {tr_loss:.4f}/{tr_acc:.4f}  |  test {te_loss:.4f}/{te_acc:.4f}  |  lr {scheduler.get_last_lr()[0]:.5f}")

    # ìµœê³  ì„±ëŠ¥ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(torch.load(save_path, map_location=device))
    return model, history

def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--samples_per_class", type=int, default=100,
                   help="í´ë˜ìŠ¤ë‹¹ ì‚¬ìš©í•  CIFAR-10 ì›ë³¸ ìƒ˜í”Œ ìˆ˜")
    p.add_argument("--use_trad_aug", action="store_true",
                   help="ì „í†µì  ì¦ê°• ì ìš© (ì›ë³¸ + ì¦ê°•)")
    p.add_argument("--num_augment", type=int, default=1,
                   help="ì›ë³¸ 1ì¥ë‹¹ ìƒì„±í•  ì¦ê°• ìˆ˜ (1ì´ë©´ 2ë°°)")
    p.add_argument("--use_sd", action="store_true",
                   help="Stable Diffusion ìƒì„± ì´ë¯¸ì§€ í¬í•¨ ì—¬ë¶€")
    p.add_argument("--epochs", type=int, default=100)
    p.add_argument("--batch_size", type=int, default=128)
    p.add_argument("--lr", type=float, default=0.05)
    p.add_argument("--model", type=str, default="resnet18")
    p.add_argument("--save_path", type=str, default="./models/best.pth")
    return p.parse_args()


def main():
    args = parse_args()
    os.makedirs(os.path.dirname(args.save_path), exist_ok=True)

    # 1ï¸âƒ£ ê¸°ë³¸ ë°ì´í„°ì…‹: few-shot CIFAR-10
    train_subset, testset = create_few_shot_cifar10(samples_per_class=args.samples_per_class)

    # 2ï¸âƒ£ ì¦ê°• ì˜µì…˜ ì²˜ë¦¬
    if args.use_trad_aug:
        # --- ì „í†µì  ì¦ê°• ì ìš© ---
        train_ds = create_augmented_dataset(train_subset, num_augment=args.num_augment)
        print(f"âœ… ì „í†µì  ì¦ê°• ì ìš© ì™„ë£Œ! (ì›ë³¸+ì¦ê°• ì´ {len(train_ds)}ì¥)")
    elif args.use_sd:
        # --- Stable Diffusion ìƒì„± ì´ë¯¸ì§€ í¬í•¨ ---
        train_ds = build_cifar10_with_sd(
            split="train",
            include_sd=True,
            samples_per_class=args.samples_per_class
        )
        print(f"âœ… ìƒì„±í˜• ì´ë¯¸ì§€ í¬í•¨ ì™„ë£Œ! (ì´ {len(train_ds)}ì¥)")
    else:
        # --- ì›ë³¸ë§Œ ---
        train_ds = train_subset
        print(f"âœ… ì›ë³¸ ë°ì´í„°ë§Œ ì‚¬ìš© ({len(train_ds)}ì¥)")

    test_ds = testset

    # 3ï¸âƒ£ ëª¨ë¸ ì¤€ë¹„
    model = get_resnet18_cifar10(num_classes=10)

    # 4ï¸âƒ£ í•™ìŠµ ì‹¤í–‰
    model, hist = train_classifier(
        model=model,
        train_dataset=train_ds,
        test_dataset=test_ds,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr,
        device="cuda",
        save_path=args.save_path
    )

    print(f"\nğŸ¯ Best test accuracy: {hist['best_acc']:.4f}")
    print(f"ğŸ“ Saved model: {args.save_path}")


if __name__ == "__main__":
    main()